{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational auto-encoders on anime faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries \n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os \n",
    "import zipfile\n",
    "import urllib.request\n",
    "import random\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prepare dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters \n",
    "\n",
    "np.random.seed(51)\n",
    "\n",
    "BATCH_SIZE = 2000\n",
    "\n",
    "LATENT_DIM = 512\n",
    "\n",
    "IMAGE_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset \n",
    "\n",
    "dataurl = \"https://storage.googleapis.com/learning-datasets/Resources/anime-faces.zip\"\n",
    "\n",
    "data_file_name = 'animefaces.zip'\n",
    "\n",
    "download_dir = 'tmp/anime/'\n",
    "\n",
    "urllib.request.urlretrieve(dataurl, data_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Zip file \n",
    "\n",
    "zip_ref = zipfile.ZipFile(data_file_name , 'r')\n",
    "\n",
    "zip_ref.extractall(download_dir)\n",
    "\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the Dataset \n",
    "\n",
    "def get_dataset_slice_paths(image_dir):\n",
    "    \n",
    "    # Returns List of path to all image files\n",
    "    \n",
    "    image_file_list = os.listdir(image_dir)\n",
    "    \n",
    "    image_paths = [os.path.join(image_dir , fname) for fname in image_file_list]\n",
    "\n",
    "    return image_paths\n",
    "\n",
    "def map_images(image_filename):\n",
    "    \n",
    "    # Preprocess the images \n",
    "    \n",
    "    img_raw = tf.io.read_file(image_filename)\n",
    "    \n",
    "    image = tf.image.decode_jpeg(img_raw)\n",
    "    \n",
    "    \n",
    "    image = tf.cast(image , dtype= tf.float32)\n",
    "    \n",
    "    image = image.resize(image , (IMAGE_SIZE , IMAGE_SIZE))\n",
    "    \n",
    "    image = image / 255.0\n",
    "    \n",
    "    image = tf.reshape(image , shape = (IMAGE_SIZE , IMAGE_SIZE , 3 , ))\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Data and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lists conataining images paths\n",
    "\n",
    "data_path = \"/tmp/anime/images/\"\n",
    "\n",
    "paths = get_dataset_slice_paths(data_path)\n",
    "\n",
    "# shuffle the paths\n",
    "\n",
    "random.shuffle(paths)\n",
    "\n",
    "# Split the path to training and validation dataset\n",
    "\n",
    "path_len = len(paths)\n",
    "\n",
    "train_path_len = int(path_len * 0.8)\n",
    "\n",
    "train_paths = paths[: train_path_len]\n",
    "\n",
    "val_paths = paths[train_path_len : ]\n",
    "\n",
    "# load the training image paths into tensors, create batches and shuffle\n",
    "\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices((train_paths)) \n",
    "\n",
    "training_dataset = training_dataset.map(map_images)\n",
    "\n",
    "training_dataset = training_dataset.shuffle(1000).batch(BATCH_SIZE)\n",
    "\n",
    "# load the validation image paths into tensors and create batches\n",
    "\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((val_paths))\n",
    "\n",
    "validation_dataset = validation_dataset.map(map_images)\n",
    "\n",
    "validation_dataset = validation_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "print(f'number of batches in the training set: {len(training_dataset)}')\n",
    "\n",
    "print(f'number of batches in the validation set: {len(validation_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_faces(dataset, size=9):\n",
    "    \n",
    "  '''Takes a sample from a dataset batch and plots it in a grid.'''\n",
    "  dataset = dataset.unbatch().take(size)\n",
    "  \n",
    "  n_cols = 3\n",
    "  \n",
    "  n_rows = size//n_cols + 1\n",
    "  \n",
    "  plt.figure(figsize=(5, 5))\n",
    "  \n",
    "  i = 0\n",
    "  \n",
    "  for image in dataset:\n",
    "      \n",
    "    i += 1\n",
    "    \n",
    "    disp_img = np.reshape(image, (64,64,3))\n",
    "    \n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    \n",
    "    plt.xticks([])\n",
    "    \n",
    "    plt.yticks([])\n",
    "    \n",
    "    plt.imshow(disp_img)\n",
    "\n",
    "\n",
    "def display_one_row(disp_images, offset, shape=(28, 28)):\n",
    "    \n",
    "  '''Displays a row of images.'''\n",
    "  \n",
    "  for idx, image in enumerate(disp_images):\n",
    "      \n",
    "    plt.subplot(3, 10, offset + idx + 1)\n",
    "    \n",
    "    plt.xticks([])\n",
    "    \n",
    "    plt.yticks([])\n",
    "    \n",
    "    image = np.reshape(image, shape)\n",
    "    \n",
    "    plt.imshow(image)\n",
    "\n",
    "\n",
    "def display_results(disp_input_images, disp_predicted):\n",
    "    \n",
    "  '''Displays input and predicted images.'''\n",
    "  \n",
    "  plt.figure(figsize=(15, 5))\n",
    "  \n",
    "  display_one_row(disp_input_images, 0, shape=(IMAGE_SIZE,IMAGE_SIZE,3))\n",
    "  \n",
    "  display_one_row(disp_predicted, 20, shape=(IMAGE_SIZE,IMAGE_SIZE,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_faces(validation_dataset, size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sampling(tf.keras.layers.layer):\n",
    "    \n",
    "    def call(self , inputs):\n",
    "        \n",
    "        mu , sigma = inputs\n",
    "        \n",
    "        batch = tf.shape(mu)[0]\n",
    "        \n",
    "        dim = tf.shape(mu)[1]\n",
    "        \n",
    "        epsilon = tf.keras.backend.random_normal(shape = (batch , dim))\n",
    "        \n",
    "        z = mu + tf.exp(0.5 * sigma) * epsilon \n",
    "        \n",
    "        return z       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define encoder layers\n",
    "\n",
    "def encoder_layers(inputs , latent_dim):\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters = 32 , kernel_size = ( 3, 3) , strides = 2 , activation = ' relu' , padding = 'same' , name = 'encode_conv1')(inputs)\n",
    "    \n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters = 64 , kernel_size = ( 3, 3) , strides = 2 , activation = 'relu' , padding = 'same' , name = 'encode_conv2')(x)\n",
    "    \n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters = 64 , kernel_size = ( 3 , 3) , strides = 2 , activation = 'relu' , padding = 'same' , name = 'encode_conv3')(x)\n",
    "    \n",
    "    batch_3 = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Flatten(name = 'encode_flatten')(batch_3)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(1024 , activation = 'relu' , name = 'encode_dense')(x)\n",
    "    \n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    mu = tf.keras.layers.Dense(latent_dim , name = 'laten_mu')(x)\n",
    "    \n",
    "    sigma = tf.keras.layers.Dense(latent_dim , name = 'laten_sigma')(x)\n",
    "    \n",
    "    return mu , sigma , batch_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Encoder model \n",
    "\n",
    "def encoder_model(input_shape):\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape = (input_shape))\n",
    "    \n",
    "    mu , sigma , conv_shape = encoder_layers(inputs , latent_dim= LATENT_DIM)\n",
    "    \n",
    "    z = sampling()((mu , sigma))\n",
    "    \n",
    "    model = tf.keras.Model(inputs , outputs = [ mu , sigma , z])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model , conv_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder layers\n",
    "\n",
    "def decoder_layers(inputs , conv_shape):\n",
    "    \n",
    "    units = conv_shape[0] * conv_shape[1] * conv_shape[2]       # number of neurons\n",
    "    \n",
    "    x = tf.keras.layers.Dense(units , activation = 'relu' , name = 'Decode_dense1')(inputs)\n",
    "    \n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.Reshape((conv_shape[0] , conv_shape[1], conv_shape[2]) , name = 'decode_shape')    # reshape the decode input\n",
    "    \n",
    "    # Upsample the image back to original dimentions\n",
    "    \n",
    "    x = tf.keras.layers.Conv2DTranspose(filters = 64 , kernel_size = (3 , 3) , strides = 2 , padding = 'same' , activation = 'relu' , name = 'decode_conv1')(x)\n",
    "    \n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2DTranspose(filters = 64 , kernel_size = (3 ,3 ) , strides = 2 , padding = 'same' , activation = 'relu' , name = 'decode_conv2')(x)\n",
    "    \n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2DTranspose(filters = 32 , kernel_size = (3 ,3 ) , strides = 2 , padding = 'same' , activation = 'relu' , name = 'decode_conv3')(x)\n",
    "  \n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2DTranspose(filters = 3 , kernel_size = (3 ,3 ) , strides = 2 , padding = 'same' , activation = 'relu' , name = 'decode_final')(x)\n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder model \n",
    "\n",
    "def decoder_model(latent_dim , conv_shape):\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape = latent_dim)\n",
    "    \n",
    "    outputs = decoder_layers(inputs , conv_shape)\n",
    "    \n",
    "    model = tf.keras.Model(inputs , outputs)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kullback–Leibler Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_reconstruciton_loss(mu , sigma):\n",
    "    \n",
    "    kl_loss = 1 + sigma - tf.square(mu) - tf.math.exp(sigma)\n",
    "    \n",
    "    return tf.reduce_mean(kl_loss) * -0.5    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting all together\n",
    "\n",
    "def vae_model(encoder , decoder , input_shape):\n",
    "    \n",
    "    inputs = tf.keras.model.Inputs(input_shape)\n",
    "    \n",
    "    mu , sigma , z = encoder(inputs)\n",
    "    \n",
    "    reconstructed = decoder(z)\n",
    "    \n",
    "    model = tf.keras.layers.Model(inputs = inputs , outputs = reconstructed)\n",
    "    \n",
    "    loss = kl_reconstruciton_loss(mu , sigma=sigma)\n",
    "    \n",
    "    model.add_loss(loss)\n",
    "\n",
    "    return model    \n",
    "      \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(latent_dim , input_shape):\n",
    "    \n",
    "    encoder, conv_shape = encoder_model(latent_dim=LATENT_DIM, input_shape=input_shape) \n",
    "    \n",
    "    decoder = decoder_model(latent_dim=latent_dim, conv_shape=conv_shape) \n",
    "    \n",
    "    vae = vae_model(encoder, decoder, input_shape=input_shape)\n",
    "    \n",
    "    return encoder , decoder , vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, decoder, vae = get_models(input_shape=(64,64,3,), latent_dim=LATENT_DIM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
